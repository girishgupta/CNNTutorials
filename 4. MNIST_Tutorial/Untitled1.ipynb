{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "image_size = 28\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display(val):\n",
    "    image = mnist.train.images[val].reshape([image_size, image_size])\n",
    "    label = mnist.train.labels[val].argmax()\n",
    "    plt.title('Training: %d  Label: %d' % (val, label))\n",
    "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nm2XK4TfL1P8B3FlXyB1msIoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x29b1aba4278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createConvolutionLayer(x_input, kernel_size, features, depth):\n",
    "    # createConvolutionLayer generates a convolution layer in the session graph\n",
    "    # by assigning weights, biases, convolution and relu function\n",
    "    #\n",
    "    # x_input - output from the previous layer\n",
    "    # kernel_size - size of the feature kernels\n",
    "    # depth - number of feature kernels\n",
    "    #\n",
    "    # returns convolution layer in graph\n",
    "    #\n",
    "    print(\"conv: input size: \" + str(x_input.get_shape()))\n",
    "    weights = tf.get_variable('weights', shape=[kernel_size, kernel_size, features, depth],\n",
    "                             initializer = tf.contrib.layers.xavier_initializer())\n",
    "    \n",
    "    biases = tf.get_variable('biases', shape=[depth], initializer=tf.constant_initializer(0))\n",
    "    print(\"shape:\" + str(x_input.get_shape()))\n",
    "    print(\"shape:\" + str(weights.get_shape()))\n",
    "    convolution = tf.nn.conv2d(x_input, weights, strides=[1,1,1,1], padding='SAME')\n",
    "    print(\"shape:\" + str(convolution.get_shape()))\n",
    "    added = tf.nn.bias_add(convolution, biases)\n",
    "    \n",
    "    return tf.nn.relu(added), weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createFullyConnectedLayer(x_input, width):\n",
    "    # createFullyConnectedLayer generates a fully connected layer in the session graph\n",
    "    # \n",
    "    # x_input - output from previous layer\n",
    "    # width - width of the layer (eg for a 10 class output you need to end with a 10 width layer\n",
    "    #\n",
    "    # returns fully connected layer in graph\n",
    "    #\n",
    "    print(\"fc: input size: \" + str(x_input.get_shape()))\n",
    "    weights = tf.get_variable('weights', shape=[x_input.get_shape()[1], width],\n",
    "                             initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable('biases', shape=[width], initializer=tf.constant_initializer(0))\n",
    "     \n",
    "    matrix_multiply = tf.matmul(x_input, weights)\n",
    "    \n",
    "    return tf.nn.bias_add(matrix_multiply, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createSoftmaxLayer(x_input, width):\n",
    "    # createSoftmaxLayer generates a softmax layer in the session graph\n",
    "    # \n",
    "    # x_input - output from previous layer\n",
    "    # width - width of the layer (eg for a 10 class output you need to end with a 10 width layer\n",
    "    #\n",
    "    # returns softmax layer in graph\n",
    "    #\n",
    "    print(\"softmax: input size: \" + str(x_input.get_shape()))\n",
    "    weights = tf.get_variable('weights', shape=[x_input.get_shape()[1], width],\n",
    "                             initializer = tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable('biases', shape=[width], initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    matrix_multiply = tf.matmul(x_input, weights)\n",
    "    \n",
    "    return tf.nn.softmax(tf.nn.bias_add(matrix_multiply, biases))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createLinearRectifier(x_input):\n",
    "    # createLinearRectifier generates a ReLu in the session graph\n",
    "    # \n",
    "    # The reason this exists is due to the last fully connected layer not needing a relu while others do\n",
    "    # x_input - output from previous layer\n",
    "    # width - width of the layer\n",
    "    #\n",
    "    # returns ReLu in graph\n",
    "    # \n",
    "    \n",
    "    return tf.nn.relu(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createPoolingLayer(x_input, kernel_size):\n",
    "    # createPoolingLayer generates a pooling layer in the session graph\n",
    "    # \n",
    "    # The reason this exists is due to the last fully connected layer not needing a relu while others do\n",
    "    # x_input - output from previous layer\n",
    "    # kernel_size - size of the kernel\n",
    "    #\n",
    "    # returns pooling layer in graph\n",
    "    # \n",
    "    print(\"pool: input size: \" + str(x_input.get_shape()))\n",
    "    return tf.nn.max_pool(x_input, ksize=[1, kernel_size, kernel_size, 1], strides=[1,kernel_size,kernel_size, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createNetwork(x_input, is_training):\n",
    "    with tf.variable_scope('conv1'):\n",
    "        print(\"shape:\" + str(x_input.get_shape()))\n",
    "        convolution_layer1, weights1 = createConvolutionLayer(x_input, 5, 1, 32)\n",
    "        print(\"shape:\" + str(convolution_layer1.get_shape()))\n",
    "        pooling_layer1 = createPoolingLayer(convolution_layer1, 2)\n",
    "    with tf.variable_scope('conv2'):\n",
    "        convolution_layer2, weights2 = createConvolutionLayer(pooling_layer1, 5, 32, 64)\n",
    "        pooling_layer1 = createPoolingLayer(convolution_layer2, 2)\n",
    "        pooling_layer1_shape = pooling_layer1.get_shape().as_list()\n",
    "        pooling_layer1_flattened = tf.reshape(pooling_layer1, [-1, pooling_layer1_shape[1] * pooling_layer1_shape[2] * pooling_layer1_shape[3]])\n",
    "    with tf.variable_scope('fc1'):\n",
    "        fully_connected_layer1 = createFullyConnectedLayer(pooling_layer1_flattened, 1024)\n",
    "        fully_connected_relu1 = createLinearRectifier(fully_connected_layer1)\n",
    "        #fully_connected_relu1 = tf.cond(is_training, lambda: tf.nn.dropout(fully_connected_relu1, keep_prob=0.5), lambda: fully_connected_relu1)\n",
    "    with tf.variable_scope('softmax'):\n",
    "        output = createSoftmaxLayer(fully_connected_relu1, 10)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:(?, 28, 28, 1)\n",
      "conv: input size: (?, 28, 28, 1)\n",
      "shape:(?, 28, 28, 1)\n",
      "shape:(5, 5, 1, 32)\n",
      "shape:(?, 28, 28, 32)\n",
      "shape:(?, 28, 28, 32)\n",
      "pool: input size: (?, 28, 28, 32)\n",
      "conv: input size: (?, 14, 14, 32)\n",
      "shape:(?, 14, 14, 32)\n",
      "shape:(5, 5, 32, 64)\n",
      "shape:(?, 14, 14, 64)\n",
      "pool: input size: (?, 14, 14, 64)\n",
      "fc: input size: (?, 3136)\n",
      "softmax: input size: (?, 1024)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    x_input = tf.placeholder(tf.float32, shape=[None, image_size * image_size])\n",
    "    y_output = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    #learning rate\n",
    "    learning_rate = 0.0001\n",
    "    \n",
    "    # get model\n",
    "    x_image = tf.reshape(x_input, [-1, 28, 28, 1])\n",
    "    prediction_output = createNetwork(x_image, is_training)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y_output,1), tf.argmax(prediction_output,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    loss_function = tf.reduce_mean(-tf.reduce_sum(y_output * tf.log(prediction_output), reduction_indices=[1]))\n",
    "\n",
    "    #optimization method\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_name = \"CNN\" + \"LRN\" + str(learning_rate) + \"EP\" + str(iterations) + \"DRP\" \n",
    "model_filename = model_name + \"Model.ckpt\"\n",
    "model_directory = os.getcwd() + \"/Models/\" + model_name\n",
    "model_path = model_directory + \"/\" + model_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING: CNNLRN0.0001EP10DRP\n",
      "step 0 Train Acc: 0.09375 Train Loss: 2.30706\n",
      "step 100 Train Acc: 0.9375 Train Loss: 0.243375\n",
      "step 200 Train Acc: 0.890625 Train Loss: 0.418127\n",
      "step 300 Train Acc: 0.9375 Train Loss: 0.153159\n",
      "step 400 Train Acc: 0.984375 Train Loss: 0.0993362\n",
      "step 500 Train Acc: 0.90625 Train Loss: 0.208997\n",
      "step 600 Train Acc: 0.953125 Train Loss: 0.221878\n",
      "step 700 Train Acc: 0.9375 Train Loss: 0.184365\n",
      "step 800 Train Acc: 0.953125 Train Loss: 0.139517\n",
      "step 900 Train Acc: 0.953125 Train Loss: 0.103535\n",
      "step 1000 Train Acc: 1.0 Train Loss: 0.0415429\n",
      "step 1100 Train Acc: 0.96875 Train Loss: 0.0545608\n",
      "step 1200 Train Acc: 1.0 Train Loss: 0.0335335\n",
      "step 1300 Train Acc: 0.96875 Train Loss: 0.106182\n",
      "step 1400 Train Acc: 0.953125 Train Loss: 0.158425\n",
      "step 1500 Train Acc: 0.984375 Train Loss: 0.0692326\n",
      "step 1600 Train Acc: 0.96875 Train Loss: 0.184793\n",
      "step 1700 Train Acc: 0.984375 Train Loss: 0.0409942\n",
      "step 1800 Train Acc: 0.984375 Train Loss: 0.0684483\n",
      "step 1900 Train Acc: 0.984375 Train Loss: 0.145716\n",
      "step 2000 Train Acc: 1.0 Train Loss: 0.00868765\n",
      "step 2100 Train Acc: 0.984375 Train Loss: 0.0478009\n",
      "step 2200 Train Acc: 0.984375 Train Loss: 0.0496077\n",
      "step 2300 Train Acc: 0.953125 Train Loss: 0.150093\n",
      "step 2400 Train Acc: 1.0 Train Loss: 0.0158729\n",
      "step 2500 Train Acc: 1.0 Train Loss: 0.0164558\n",
      "step 2600 Train Acc: 0.984375 Train Loss: 0.020268\n",
      "step 2700 Train Acc: 0.984375 Train Loss: 0.0561414\n",
      "step 2800 Train Acc: 1.0 Train Loss: 0.0131549\n",
      "step 2900 Train Acc: 0.984375 Train Loss: 0.0408478\n",
      "step 3000 Train Acc: 0.984375 Train Loss: 0.0551625\n",
      "step 3100 Train Acc: 0.96875 Train Loss: 0.0397075\n",
      "step 3200 Train Acc: 0.984375 Train Loss: 0.0532815\n",
      "step 3300 Train Acc: 1.0 Train Loss: 0.0250584\n",
      "step 3400 Train Acc: 0.984375 Train Loss: 0.044814\n",
      "step 3500 Train Acc: 0.984375 Train Loss: 0.0543625\n",
      "step 3600 Train Acc: 1.0 Train Loss: 0.00761337\n",
      "step 3700 Train Acc: 0.984375 Train Loss: 0.0252476\n",
      "step 3800 Train Acc: 1.0 Train Loss: 0.00892074\n",
      "step 3900 Train Acc: 0.96875 Train Loss: 0.0770703\n",
      "step 4000 Train Acc: 0.984375 Train Loss: 0.0228147\n",
      "step 4100 Train Acc: 1.0 Train Loss: 0.0108861\n",
      "step 4200 Train Acc: 1.0 Train Loss: 0.0022996\n",
      "step 4300 Train Acc: 1.0 Train Loss: 0.0166701\n",
      "step 4400 Train Acc: 0.984375 Train Loss: 0.0216128\n",
      "step 4500 Train Acc: 0.984375 Train Loss: 0.0566097\n",
      "step 4600 Train Acc: 0.96875 Train Loss: 0.0918818\n",
      "step 4700 Train Acc: 0.984375 Train Loss: 0.0296459\n",
      "step 4800 Train Acc: 0.984375 Train Loss: 0.0491513\n",
      "step 4900 Train Acc: 1.0 Train Loss: 0.00857675\n",
      "step 5000 Train Acc: 1.0 Train Loss: 0.00908164\n",
      "step 5100 Train Acc: 1.0 Train Loss: 0.0106937\n",
      "step 5200 Train Acc: 1.0 Train Loss: 0.00197084\n",
      "step 5300 Train Acc: 1.0 Train Loss: 0.0105322\n",
      "step 5400 Train Acc: 0.984375 Train Loss: 0.0146477\n",
      "step 5500 Train Acc: 1.0 Train Loss: 0.0162458\n",
      "step 5600 Train Acc: 0.984375 Train Loss: 0.0871873\n",
      "step 5700 Train Acc: 1.0 Train Loss: 0.0218819\n",
      "step 5800 Train Acc: 1.0 Train Loss: 0.00256237\n",
      "step 5900 Train Acc: 1.0 Train Loss: 0.006669\n",
      "step 6000 Train Acc: 1.0 Train Loss: 0.0187096\n",
      "step 6100 Train Acc: 1.0 Train Loss: 0.0109166\n",
      "step 6200 Train Acc: 0.984375 Train Loss: 0.0291659\n",
      "step 6300 Train Acc: 0.984375 Train Loss: 0.0182587\n",
      "step 6400 Train Acc: 0.984375 Train Loss: 0.104243\n",
      "step 6500 Train Acc: 1.0 Train Loss: 0.0118579\n",
      "step 6600 Train Acc: 0.984375 Train Loss: 0.0414673\n",
      "step 6700 Train Acc: 1.0 Train Loss: 0.00793854\n",
      "step 6800 Train Acc: 0.984375 Train Loss: 0.0275126\n",
      "step 6900 Train Acc: 1.0 Train Loss: 0.00179845\n",
      "step 7000 Train Acc: 1.0 Train Loss: 0.00907648\n",
      "step 7100 Train Acc: 1.0 Train Loss: 0.0153778\n",
      "step 7200 Train Acc: 1.0 Train Loss: 0.00899721\n",
      "step 7300 Train Acc: 0.984375 Train Loss: 0.0451482\n",
      "step 7400 Train Acc: 1.0 Train Loss: 0.00128092\n",
      "step 7500 Train Acc: 1.0 Train Loss: 0.00439436\n",
      "step 7600 Train Acc: 0.984375 Train Loss: 0.0389464\n",
      "step 7700 Train Acc: 1.0 Train Loss: 0.00641363\n",
      "step 7800 Train Acc: 1.0 Train Loss: 0.00403987\n",
      "step 7900 Train Acc: 1.0 Train Loss: 0.000749833\n",
      "step 8000 Train Acc: 0.984375 Train Loss: 0.0421943\n",
      "step 8100 Train Acc: 0.984375 Train Loss: 0.0525762\n",
      "step 8200 Train Acc: 1.0 Train Loss: 0.00184914\n",
      "step 8300 Train Acc: 0.984375 Train Loss: 0.0153881\n",
      "step 8400 Train Acc: 0.984375 Train Loss: 0.0204637\n",
      "step 8500 Train Acc: 1.0 Train Loss: 0.00161148\n",
      "step 8600 Train Acc: 1.0 Train Loss: 0.000856753\n",
      "step 8700 Train Acc: 1.0 Train Loss: 0.00513432\n",
      "step 8800 Train Acc: 1.0 Train Loss: 0.0112206\n",
      "step 8900 Train Acc: 1.0 Train Loss: 0.00309559\n",
      "step 9000 Train Acc: 1.0 Train Loss: 0.00310351\n",
      "step 9100 Train Acc: 1.0 Train Loss: 0.00350241\n",
      "step 9200 Train Acc: 1.0 Train Loss: 0.00279734\n",
      "step 9300 Train Acc: 1.0 Train Loss: 0.00736159\n",
      "step 9400 Train Acc: 1.0 Train Loss: 0.00240812\n",
      "step 9500 Train Acc: 0.984375 Train Loss: 0.0435124\n",
      "step 9600 Train Acc: 0.984375 Train Loss: 0.0152139\n",
      "step 9700 Train Acc: 1.0 Train Loss: 0.0057251\n",
      "step 9800 Train Acc: 1.0 Train Loss: 0.00917773\n",
      "step 9900 Train Acc: 1.0 Train Loss: 0.00522975\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1b6670c0bad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_output\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m \u001b[0mx_input\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_output\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Final: \"\u001b[0m \u001b[1;34m\" Test Acc: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_accuracy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" Test Loss: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "start = time.time()\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "time_list = []\n",
    "epoch_list = []\n",
    "print(\"TRAINING: \" + model_name)\n",
    "with tf.Session(graph = graph) as session:\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    if os.path.exists(model_directory):\n",
    "            load_path = saver.restore(session, model_path)\n",
    "    for i in range(10000):\n",
    "        batch = mnist.train.next_batch(batch_size)\n",
    "        if i%100 == 0:\n",
    "            feed_dict = {x_input:batch[0], y_output: batch[1], is_training: False}\n",
    "            train_accuracy = accuracy.eval(feed_dict=feed_dict)\n",
    "            train_loss = session.run(loss_function, feed_dict=feed_dict)\n",
    "            print(\"step \" + str(i) + \" Train Acc: \" + str(train_accuracy) + \" Train Loss: \" + str(train_loss))\n",
    "        # Train system\n",
    "        session.run([optimizer], feed_dict={x_input: batch[0], y_output: batch[1], is_training: True})\n",
    "    train_loss = session.run(loss_function, feed_dict = {x_input: mnist.test.images, y_output: mnist.test.labels, is_training: False})\n",
    "    test_accuracy = accuracy.eval(feed_dict={ x_input: mnist.test.images, y_output: mnist.test.labels, is_training: False})\n",
    "    print(\"Final: \" \" Test Acc: \" + str(test_accuracy) + \" Test Loss: \" + str(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a Pandas dataframe from some data.\n",
    "df = pd.DataFrame({'Data': [p]})\n",
    "\n",
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter(model_path + 'TrainLossAcc.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Convert the dataframe to an XlsxWriter Excel object.\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
