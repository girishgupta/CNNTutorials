{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Keypoint Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "from scipy import misc\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_directory = \"/data/\"\n",
    "training_filename = \"training.csv\"\n",
    "batch_size = 128\n",
    "num_epochs= 1000\n",
    "dropout = True\n",
    "use_ConvNet = False\n",
    "xavier_weights = False\n",
    "learning_amount= .0001\n",
    "momentum_amount = 0.9\n",
    "learn_decay = False\n",
    "momentum_inc = False\n",
    "opt = \"MOM\"\n",
    "\n",
    "model_name = \"ConvNet\" + str(learning_amount) + \"_\" + str(num_epochs) + opt + \"ReallyZeroWeights\"\n",
    "model_filename = model_name + \".ckpt\"\n",
    "model_directory = os.getcwd() + \"/FinalModels/\" + model_name\n",
    "model_path = model_directory + \"/\" + model_filename\n",
    "\n",
    "# Images are 96 x 96 grayscale with 15 features using (x, y) coordinates\n",
    "image_size = 96\n",
    "num_channels = 1 # grayscale\n",
    "num_classes = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class MiniBatcher(object):\n",
    "    def __init__(self, batch_size, shuffle=False, seed=43):\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.random = np.random.RandomState(seed)\n",
    "\n",
    "    def __call__(self, X, y=None):\n",
    "        if self.shuffle:\n",
    "            _shuffle_arrays([X, y] if y is not None else [X], self.random)\n",
    "        self.X, self.y = X, y\n",
    "        return self\n",
    "\n",
    "        \n",
    "    def __iter__(self):\n",
    "        bs = self.batch_size\n",
    "        for i in range((self.n_samples + bs - 1) // bs):\n",
    "            sl = slice(i * bs, (i + 1) * bs)\n",
    "            Xb = _sldict(self.X, sl)\n",
    "            if self.y is not None:\n",
    "                yb = _sldict(self.y, sl)\n",
    "            else:\n",
    "                yb = None\n",
    "            yield self.transform(Xb, yb)\n",
    "            \n",
    "\n",
    "    @property\n",
    "    def n_samples(self):\n",
    "        X = self.X\n",
    "        if isinstance(X, dict):\n",
    "            return len(list(X.values())[0])\n",
    "        else:\n",
    "            return len(X)\n",
    "\n",
    "    def transform(self, Xb, yb):\n",
    "        return Xb, yb\n",
    "\n",
    "    def __getstate__(self):\n",
    "        state = dict(self.__dict__)\n",
    "        for attr in ('X', 'y',):\n",
    "            if attr in state:\n",
    "                del state[attr]\n",
    "        return state\n",
    "    \n",
    "def _shuffle_arrays(arrays, random):\n",
    "    rstate = random.get_state()\n",
    "    for array in arrays:\n",
    "        if isinstance(array, dict):\n",
    "            for v in list(array.values()):\n",
    "                random.set_state(rstate)\n",
    "                random.shuffle(v)\n",
    "        else:\n",
    "            random.set_state(rstate)\n",
    "            random.shuffle(array)\n",
    "            \n",
    "def _sldict(arr, sl):\n",
    "    if isinstance(arr, dict):\n",
    "        return {k: v[sl] for k, v in arr.items()}\n",
    "    else:\n",
    "        return arr[sl]\n",
    "    \n",
    "class AugBatcher(MiniBatcher):\n",
    "\n",
    "    def __init__(self, batch_size, shuffle=False, seed=1, add_flips=False, add_rotate=False):\n",
    "        super(AugBatcher, self).__init__(batch_size, shuffle=False, seed=1)\n",
    "        self.add_flips = add_flips\n",
    "        self.add_rotate = add_rotate\n",
    "        self.flip_indices = [\n",
    "        (0, 2), (1, 3),\n",
    "        (4, 8), (5, 9), (6, 10), (7, 11),\n",
    "        (12, 16), (13, 17), (14, 18), (15, 19),\n",
    "        (22, 24), (23, 25),\n",
    "        ]\n",
    "\n",
    "    def transform(self, Xb, yb):\n",
    "        Xb, yb = super(AugBatcher, self).transform(Xb, yb)\n",
    "\n",
    "        Xb = copy.deepcopy(Xb)\n",
    "        yb = copy.deepcopy(yb)\n",
    "        if self.add_flips:\n",
    "        # Flip half of the images in this batch at random:\n",
    "            bs = Xb.shape[0]\n",
    "            indices = np.random.choice(bs, int(bs / 2), replace=False)\n",
    "            Xb[indices] = Xb[indices, :, ::-1, :]\n",
    "\n",
    "            if yb is not None:\n",
    "                # Horizontal flip of all x coordinates:\n",
    "                yb[indices, ::2] = yb[indices, ::2] * -1\n",
    "\n",
    "                # Swap places, e.g. left_eye_center_x -> right_eye_center_x\n",
    "                for a, b in self.flip_indices:\n",
    "                    yb[indices, a], yb[indices, b] = (\n",
    "                        yb[indices, b], yb[indices, a])\n",
    "        \n",
    "        #print(\"Before: \" + str(Xb[0]))\n",
    "        if self.add_rotate:\n",
    "            for i in range(0, len(Xb)): \n",
    "                if random.uniform(0, 1) > .5:\n",
    "                    angle = random.uniform(-5, 5)\n",
    "                    Xb[i] = np.reshape(misc.imrotate(np.reshape(Xb[i], [image_size, image_size]), -angle), [1, image_size, image_size, 1]) / 255\n",
    "                    for j in range(0, 30, 2):\n",
    "                        yb[i, j], yb[i, j+1] = rotatePoint((0, 0), (yb[i, j], yb[i, j+1]), angle)\n",
    "        \n",
    "        #print(\"After: \" + str(Xb[0]))\n",
    "        return Xb, yb\n",
    "\n",
    "def rotatePoint(centerPoint,point,angle):\n",
    "    angle = math.radians(angle)\n",
    "    temp_point = point[0]-centerPoint[0] , point[1]-centerPoint[1]\n",
    "    temp_point = ( temp_point[0]*math.cos(angle)-temp_point[1]*math.sin(angle) , temp_point[0]*math.sin(angle)+temp_point[1]*math.cos(angle))\n",
    "    temp_point = temp_point[0]+centerPoint[0] , temp_point[1]+centerPoint[1]\n",
    "    return temp_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(filename):\n",
    "    \n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep = ' '))\n",
    "    df = df.dropna()\n",
    "\n",
    "    values = np.vstack(df['Image'].values) / 255.\n",
    "    values = values.astype(np.float32)\n",
    "\n",
    "    labels = df[df.columns[:-1]].values\n",
    "    labels = (labels - 48) / 48  \n",
    "    values, labels = shuffle(values, labels, random_state=3)  \n",
    "    labels = labels.astype(np.float32)\n",
    "\n",
    "    return values, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_values, image_labels = loadData(os.getcwd() + data_directory + training_filename)\n",
    "image_values = np.reshape(image_values, [-1, image_size, image_size, 1])\n",
    "\n",
    "train_data, validation_data, train_labels, validation_labels = train_test_split(image_values, image_labels, 0.7)\n",
    "validation_data, test_data, validation_labels, test_labels = train_test_split(validation_data, validation_labels, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createFullyConnectedLayer(x_input, width):\n",
    "    # createFullyConnectedLayer generates a fully connected layer in the session graph\n",
    "    # \n",
    "    # x_input - output from previous layer\n",
    "    # width - width of the layer (eg for a 10 class output you need to end with a 10 width layer\n",
    "    #\n",
    "    # returns fully connected layer in graph\n",
    "    #\n",
    "    if(xavier_weights):\n",
    "        print(\"Xavier Weights\")\n",
    "        weights = tf.get_variable('weights', shape=[x_input.get_shape()[1], width],\n",
    "                             initializer = tf.contrib.layers.xavier_initializer())\n",
    "    else:\n",
    "        print(\"Zero Weights\")\n",
    "        weights = tf.get_variable('weights', shape=[x_input.get_shape()[1], width],\n",
    "                             initializer=tf.constant_initializer(0))\n",
    "        \n",
    "    biases = tf.get_variable('biases', shape=[width], initializer=tf.constant_initializer(0))\n",
    "     \n",
    "    matrix_multiply = tf.matmul(x_input, weights)\n",
    "    \n",
    "    total_parameters = x_input.get_shape()[1] * width + width\n",
    "    \n",
    "    print(\"Created Fully Connected Layer: Input\" + str(x_input.get_shape()) + \" Parameters(\" + str(total_parameters) + \")\")\n",
    "    \n",
    "    return tf.nn.bias_add(matrix_multiply, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createConvolutionLayer(x_input, kernel_size, features, depth):\n",
    "    # createConvolutionLayer generates a convolution layer in the session graph\n",
    "    # by assigning weights, biases, convolution and relu function\n",
    "    #\n",
    "    # x_input - output from the previous layer\n",
    "    # kernel_size - size of the feature kernels\n",
    "    # depth - number of feature kernels\n",
    "    #\n",
    "    # returns convolution layer in graph\n",
    "    #\n",
    "    if xavier_weights:\n",
    "        print(\"Xavier Weights\")\n",
    "        weights = tf.get_variable('weights', shape=[kernel_size, kernel_size, features, depth],\n",
    "                             initializer = tf.contrib.layers.xavier_initializer())\n",
    "    else:\n",
    "        print(\"Zero Weights\")\n",
    "        weights = tf.get_variable('weights', shape=[kernel_size, kernel_size, features, depth],\n",
    "                             initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    biases = tf.get_variable('biases', shape=[depth], initializer=tf.constant_initializer(0))\n",
    "    \n",
    "    convolution = tf.nn.conv2d(x_input, weights, strides=[1,1,1,1], padding='SAME')\n",
    "    \n",
    "    added = tf.nn.bias_add(convolution, biases)\n",
    "    \n",
    "    total_parameters = kernel_size*kernel_size*features*depth + depth\n",
    "    print(\"Created Convolution Layer: Input\" + str(x_input.get_shape()) + \" Parameters(\" + str(total_parameters) + \")\")\n",
    "    \n",
    "    return tf.nn.relu(added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createLinearRectifier(x_input):\n",
    "    # createLinearRectifier generates a ReLu in the session graph\n",
    "    # \n",
    "    # The reason this exists is due to the last fully connected layer not needing a relu while others do\n",
    "    # x_input - output from previous layer\n",
    "    # width - width of the layer\n",
    "    #\n",
    "    # returns ReLU in graph\n",
    "    # \n",
    "    print(\"Created RELU Activation Function\")\n",
    "    return tf.nn.relu(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createPoolingLayer(x_input, kernel_size):\n",
    "    # createPoolingLayer generates a pooling layer in the session graph\n",
    "    # \n",
    "    # The reason this exists is due to the last fully connected layer not needing a relu while others do\n",
    "    # x_input - output from previous layer\n",
    "    # kernel_size - size of the kernel\n",
    "    #\n",
    "    # returns pooling layer in graph\n",
    "    # \n",
    "    \n",
    "    print(\"Created Pooling Layer: Downsample:\" + str(kernel_size))\n",
    "    return tf.nn.max_pool(x_input, ksize=[1, kernel_size, kernel_size, 1], strides=[1,kernel_size,kernel_size, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createSimpleNetwork(model_input):\n",
    "    with tf.variable_scope('input'):\n",
    "        input_layer = tf.reshape(model_input, [-1, image_size * image_size])\n",
    "    with tf.variable_scope('hidden'):\n",
    "        hidden_fully_connected_layer = createFullyConnectedLayer(input_layer, 100)\n",
    "    relu_layer = createLinearRectifier(hidden_fully_connected_layer)\n",
    "    with tf.variable_scope('out'):\n",
    "        model_output = createFullyConnectedLayer(relu_layer, 30)\n",
    "        \n",
    "    print(\"Simple Network Created\")\n",
    "    return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createConvNetwork(x_input, is_train):\n",
    "    # Define convolution layers\n",
    "    with tf.variable_scope('conv1'):\n",
    "        convolution_layer1 = createConvolutionLayer(x_input, 3, 1, 32)\n",
    "        pooling_layer1 = createPoolingLayer(convolution_layer1, 2)\n",
    "        # Determine if used for training or test/validate. Only use dropout for training\n",
    "        pooling_layer1 = tf.cond(is_train, lambda: tf.nn.dropout(pooling_layer1, keep_prob = 0.9 if dropout else 1.0), lambda: pooling_layer1)\n",
    "    with tf.variable_scope('conv2'):\n",
    "        convolution_layer2 = createConvolutionLayer(pooling_layer1, 2, 32, 64)\n",
    "        pooling_layer2 = createPoolingLayer(convolution_layer2, 2)\n",
    "        # Determine if used for training or test/validate. Only use dropout for training\n",
    "        pooling_layer2 = tf.cond(is_train, lambda: tf.nn.dropout(pooling_layer2, keep_prob = 0.8 if dropout else 1.0), lambda: pooling_layer2)\n",
    "    with tf.variable_scope('conv3'):\n",
    "        convolution_layer3 = createConvolutionLayer(pooling_layer2, 2, 64, 128)\n",
    "        pooling_layer3 = createPoolingLayer(convolution_layer3, 2)\n",
    "        # Determine if used for training or test/validate. Only use dropout for training\n",
    "        pooling_layer3 = tf.cond(is_train, lambda: tf.nn.dropout(pooling_layer3, keep_prob = 0.7 if dropout else 1.0), lambda: pooling_layer3)\n",
    "    \n",
    "    # Flatten output to connect to fully connected layers\n",
    "    print(\"fc: input size before flattening: \" + str(pooling_layer3.get_shape()))\n",
    "    pooling_layer3_shape = pooling_layer3.get_shape().as_list()\n",
    "    pooling_layer3_flattened = tf.reshape(pooling_layer3, [-1, pooling_layer3_shape[1] * pooling_layer3_shape[2] * pooling_layer3_shape[3]])\n",
    "    \n",
    "    # Define fully connected layers\n",
    "    with tf.variable_scope('fc1'):\n",
    "        fully_connected_layer1 = createFullyConnectedLayer(pooling_layer3_flattened, 1000)\n",
    "        fully_connected_relu1 = createLinearRectifier(fully_connected_layer1)\n",
    "        fully_connected_relu1 = tf.cond(is_train, lambda: tf.nn.dropout(fully_connected_relu1, keep_prob = 0.5 if dropout else 1.0), lambda: fully_connected_relu1)\n",
    "    with tf.variable_scope('fc2'):\n",
    "        fully_connected_layer2 = createFullyConnectedLayer(fully_connected_relu1, 1000)\n",
    "        fully_connected_relu2 = createLinearRectifier(fully_connected_layer2)\n",
    "    with tf.variable_scope('out'):\n",
    "        output = createFullyConnectedLayer(fully_connected_relu2, 30)\n",
    "        print(\"out: \" + str(output.get_shape()))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainModel(train_images, train_labels, valid_images, valid_labels, test_images, test_labels, num_epochs, batch_size, model_name, flip, rotate):\n",
    "    start = time.time()\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    time_list = []\n",
    "    epoch_list = []\n",
    "    print(\"TRAINING: \" + model_name)\n",
    "\n",
    "    with tf.Session(graph = graph) as session:\n",
    "\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        #if os.path.exists(model_directory):\n",
    "        #    print(\"Loading model...\")\n",
    "        #    load_path = saver.restore(session, model_path)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            batch = AugBatcher(batch_size=batch_size, shuffle=True, add_flips=flip, add_rotate=rotate)\n",
    "            for batch_data, batch_labels in batch(train_images, train_labels):\n",
    "                feed_dict = {model_input: batch_data, model_output: batch_labels, model_training: True}\n",
    "                # train model\n",
    "                session.run([model_optimizer], feed_dict = feed_dict)\n",
    "\n",
    "            #Store train and validation losses\n",
    "            if epoch % 10 == 0:\n",
    "                train_loss = getLoss(train_images, train_labels, session)\n",
    "                train_loss_list.append(train_loss)\n",
    "                valid_loss = getLoss(valid_images, valid_labels, session)\n",
    "                valid_loss_list.append(valid_loss)\n",
    "\n",
    "                current_time = time.time() - start\n",
    "                hours, minutes, seconds = getTime(current_time)\n",
    "                if learn_decay:\n",
    "                    print(\"Epoch[%4d]\" % epoch + \"%d\" % hours + \":%2d\" % minutes + \":%2d \" % seconds + \"%f \" % train_loss + \" %f\" % valid_loss + \" %f\" % learning_rate.eval())\n",
    "                else:\n",
    "                    print(\"Epoch[%4d]\" % epoch + \"%d\" % hours + \":%2d\" % minutes + \":%2d \" % seconds + \"%f \" % train_loss + \" %f\" % valid_loss + \" %f\" % learning_rate)\n",
    "                \n",
    "                time_list.append(current_time)\n",
    "                epoch_list.append(current_epoch)\n",
    "\n",
    "            if epoch % 300 == 0:\n",
    "                if not os.path.exists(model_directory):\n",
    "                    os.mkdir(model_directory)\n",
    "                print(\"Saving Model...\")    \n",
    "                save_path = saver.save(session, model_path)\n",
    "            # Evaluate on test dataset.\n",
    "            step = session.run(increment_global_step_op)\n",
    "        test_loss = getLoss(test_images, test_labels, session)\n",
    "        print(\" Test score: %.3f (loss = %.8f)\" % (np.sqrt(test_loss) * 48.0, test_loss)) #RMSE\n",
    "        if not os.path.exists(model_directory):\n",
    "            os.mkdir(model_directory)\n",
    "        print(\"Saving Model...\")\n",
    "        save_path = saver.save(session, model_path)\n",
    "        saveModelHistory(epoch_list, time_list, train_loss_list, valid_loss_list, 'Loss3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLoss(values, labels, session):\n",
    "    loss_ = []\n",
    "    batch = MiniBatcher(batch_size = 128)\n",
    "    for batch_values, batch_labels in batch(values, labels):\n",
    "        loss_batch = session.run(model_loss, feed_dict = {model_input : batch_values, model_output : batch_labels, model_training : False})\n",
    "        loss_.append(loss_batch)\n",
    "    return np.mean(loss_)\n",
    "\n",
    "def getTime(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    \n",
    "    return h, m, s\n",
    "\n",
    "def saveModelHistory(epoch_list, time_list, train_loss_list, valid_loss_list, name):\n",
    "    df = pd.DataFrame({'Epochs' : epoch_list, 'Time' :  time_list, 'Train': train_loss_list, 'Valid' : valid_loss_list})\n",
    "    writer = pd.ExcelWriter(model_path + name + '.xlsx', engine='xlsxwriter')\n",
    "    df.to_excel(writer, sheet_name='Sheet1')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    model_input = tf.placeholder(tf.float32, shape=(None, image_size, image_size, 1))\n",
    "    model_output = tf.placeholder(tf.float32, shape=(None, 30))\n",
    "    model_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    current_epoch = tf.Variable(0, trainable=False)\n",
    "    increment_global_step_op = tf.assign(current_epoch, current_epoch+1)\n",
    "    \n",
    "    #Parameters\n",
    "\n",
    "    if learn_decay:\n",
    "        print(\"learn decay on\")\n",
    "        learning_rate = tf.train.exponential_decay(learning_amount, current_epoch, decay_steps=num_epochs, decay_rate=.9)\n",
    "    else:\n",
    "        learning_rate = learning_amount\n",
    "    if momentum_inc:\n",
    "        m_min = 0.9\n",
    "        m_max = 0.99\n",
    "        print(\"momentum increase on\")\n",
    "        momentum_rate = m_min + (m_max - m_min) * (current_epoch / num_epochs)\n",
    "    else:\n",
    "        momentum_rate = momentum_amount\n",
    "    # get model\n",
    "    if dropout:\n",
    "        print(\"Dropout on\")\n",
    "    \n",
    "    if(use_ConvNet):\n",
    "        with tf.variable_scope(model_name):\n",
    "            model_predictions = createConvNetwork(model_input, model_training)\n",
    "    else:\n",
    "        with tf.variable_scope(model_name):\n",
    "            model_predictions = createSimpleNetwork(model_input)\n",
    "    \n",
    "    model_loss = tf.reduce_mean(tf.square(model_predictions - model_output))\n",
    "    \n",
    "    if opt == \"MOM\":\n",
    "        print(\"Using Momentum Optimizer\")\n",
    "        model_optimizer = tf.train.MomentumOptimizer(learning_rate, momentum_rate, use_nesterov=True).minimize(model_loss)\n",
    "    elif opt == \"ADAM\":\n",
    "        print(\"Using ADAM Optimizer\")\n",
    "        model_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "    else:\n",
    "        print(\"Using SGD Optimizer\")\n",
    "        model_optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_loss)\n",
    "\n",
    "trainModel(x_train, y_train, x_valid, y_valid, x_test, y_test, num_epochs, batch_size, model_name, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getLoss(values, labels, session):\n",
    "    loss_ = []\n",
    "    batch = BatchIterator(batch_size = 128)\n",
    "    for batch_values, batch_labels in batch(values, labels):\n",
    "        loss_batch = session.run(model_loss, feed_dict = {model_input : batch_values, model_output : batch_labels, model_training : False})\n",
    "        loss_.append(loss_batch)\n",
    "    return np.mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTime(seconds):\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    \n",
    "    return h, m, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainModel(x_train, y_train, x_valid, y_valid, x_test, y_test, num_epochs, batch_size, model_name, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(x_valid)):\n",
    "    for j in range(len(x_test)):\n",
    "        if np.array_equal(x_valid[i], x_test[j]):\n",
    "            print(\"CROSS CONTAMINATED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Predictions and output to screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    load_path = saver.restore(session, model_path)\n",
    "    #p = session.run(model_predictions, feed_dict={model_input: np.reshape(new_image, [1, 96, 96, 1]), model_training:False})\n",
    "    p = session.run(model_predictions, feed_dict={model_input: x_test, model_training:False})\n",
    "    #loss = session.run(loss_function, feed_dict={tf_x_batch: x_valid, y_output: y_valid, is_training:False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(15, 150))\n",
    "\n",
    "for i in range(300):\n",
    "    axis = fig.add_subplot(60, 5, i + 1, xticks=[], yticks=[])\n",
    "    img = x_test[i]\n",
    "    img = img.reshape(96, 96)\n",
    "    y = y_test[i]\n",
    "    y2 = p[i]\n",
    "    axis.imshow(img, cmap='gray')\n",
    "    # Actual labels\n",
    "    axis.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker='x', s=10, color='red')\n",
    "    # Predicted labels\n",
    "    axis.scatter(y2[0::2] * 48 + 48, y2[1::2] * 48 + 48, marker='x', s=10, color='lime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "#fig.subplots_adjust(\n",
    "#    left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "    axis = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n",
    "    img = new_image\n",
    "    img = img.reshape(96, 96)\n",
    "    #y = y_test[i]\n",
    "    y2 = p[0]\n",
    "    axis.imshow(img, cmap='gray')\n",
    "    # Actual labels\n",
    "    #axis.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker='x', s=10, color='red')\n",
    "    # Predicted labels\n",
    "    axis.scatter(y2[0::2] * 48 + 48, y2[1::2] * 48 + 48, marker='x', s=10, color='lime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "#fig.subplots_adjust(\n",
    "#    left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(25):\n",
    "    axis = fig.add_subplot(5, 5, i + 1, xticks=[], yticks=[])\n",
    "    img = new_image\n",
    "    img = img.reshape(96, 96)\n",
    "    #y = y_test[i]\n",
    "    y2 = p[0]\n",
    "    axis.imshow(img, cmap='gray')\n",
    "    # Actual labels\n",
    "    #axis.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker='x', s=10, color='red')\n",
    "    # Predicted labels\n",
    "    axis.scatter(y2[0::2] * 48 + 48, y2[1::2] * 48 + 48, marker='x', s=10, color='lime')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert New Image and Predict Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "new_image = mpimg.imread('sethdecker6.jpg')\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "#new_image = rgb2gray(new_image)\n",
    "plt.imshow(new_image)\n",
    "\n",
    "new_image = new_image / 255\n",
    "\n",
    "plt.imshow(new_image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation for Checking Model Robustness "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kfolds = 5\n",
    "flip = True\n",
    "dropout = True\n",
    "use_ConvNet = True\n",
    "num_epochs = 1000\n",
    "\n",
    "learning_amount= 0.12\n",
    "momentum_amount = 0.9\n",
    "learn_decay = True\n",
    "momentum_inc = True\n",
    "opt = \"MOM\"\n",
    "\n",
    "skf = KFold(n=len(image_values), n_folds=kfolds)\n",
    "i = 0\n",
    "for train_index, test_index in skf:\n",
    "\n",
    "    learning_rate = learning_amount\n",
    "    momentum_rate = momentum_rate\n",
    "    train_images1 = np.array([image_values[i] for i in train_index])\n",
    "    train_labels1 = np.array([image_labels[i] for i in train_index])\n",
    "    \n",
    "    test_images = np.array([image_values[i] for i in test_index])\n",
    "    test_labels = np.array([image_labels[i] for i in test_index])\n",
    "    \n",
    "    \n",
    "    print(train_images1[0].shape)\n",
    "    graph = tf.Graph()\n",
    "\n",
    "    with graph.as_default():\n",
    "\n",
    "        noise = False\n",
    "        bright= False\n",
    "        rotate = False\n",
    "\n",
    "        model_input = tf.placeholder(tf.float32, shape=(None, image_size, image_size, 1))\n",
    "        model_output = tf.placeholder(tf.float32, shape=(None, 30))\n",
    "        model_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        current_epoch = tf.Variable(0, trainable=False)\n",
    "        increment_global_step_op = tf.assign(current_epoch, current_epoch+1)\n",
    "        #Define Parameters\n",
    "\n",
    "\n",
    "        if learn_decay:\n",
    "            print(\"learn decay on\")\n",
    "            learning_rate = tf.train.exponential_decay(learning_amount, current_epoch, decay_steps=num_epochs, decay_rate=0.9)\n",
    "        else:\n",
    "            learning_rate = learning_amount\n",
    "        if momentum_inc:\n",
    "            m_min = 0.9\n",
    "            m_max = 0.99\n",
    "            print(\"momentum increase on\")\n",
    "            momentum_rate = m_min + (m_max - m_min) * (current_epoch / num_epochs)\n",
    "        else:\n",
    "            momentum_rate = momentum_amount\n",
    "            \n",
    "        #Model Name\n",
    "        model_name = \"ConvNet-CV\" + \"_Best\" + str(i) + str(\"_LRN\") + str(learning_amount) + \"_Epochs\" + str(num_epochs)\n",
    "        model_filename = model_name + \".ckpt\"\n",
    "        model_directory = os.getcwd() + \"/Models5/\" + model_name\n",
    "        model_path = model_directory + \"/\" + model_filename\n",
    "            \n",
    "        # get model\n",
    "        if(use_ConvNet):\n",
    "            with tf.variable_scope(model_name):\n",
    "                model_predictions = createConvNetwork(model_input, model_training)\n",
    "        else:\n",
    "            with tf.variable_scope(model_name):\n",
    "                model_predictions = createSimpleNetwork(model_input)\n",
    "\n",
    "        model_loss = tf.reduce_mean(tf.square(model_predictions - model_output))\n",
    "\n",
    "        if opt == \"MOM\":\n",
    "            print(\"Using Momentum Optimizer\")\n",
    "            model_optimizer = tf.train.MomentumOptimizer(learning_rate, momentum_rate, use_nesterov=True).minimize(model_loss)\n",
    "        elif opt == \"ADAM\":\n",
    "            print(\"Using ADAM Optimizer\")\n",
    "            model_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_loss)\n",
    "        else:\n",
    "            print(\"Using SGD Optimizer\")\n",
    "            model_optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_loss)\n",
    "\n",
    "        flip = True\n",
    "\n",
    "        trainModel(train_images1, train_labels1, test_images, test_labels, test_images, test_labels, num_epochs, batch_size, model_name, True, False, False, False)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfolds = 3\n",
    "#kfolds loop\n",
    "#new name\n",
    "#train\n",
    "print(\"hello\")\n",
    "momentum_steps = [0, 1, 2] \n",
    "\n",
    "learning_steps = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "learn_decay = False\n",
    "dropout = False\n",
    "use_ConvNet = False\n",
    "num_epochs = 1000\n",
    "\n",
    "skf = KFold(n_splits=kfolds, random_state=None, shuffle=False)\n",
    "i = 0\n",
    "for train_index, test_index in skf.split(image_values):\n",
    "    print(i)\n",
    "    kfolds2 = 4\n",
    "    momentum_rate = momentum_steps[i]\n",
    "    train_images1 = [image_values[i] for i in train_index]\n",
    "    train_labels1 = [image_labels[i] for i in train_index]\n",
    "    \n",
    "    test_images = [image_values[i] for i in test_index]\n",
    "    test_labels = [image_labels[i] for i in test_index]\n",
    "    j = 0\n",
    "    skf2 = KFold(n_splits=kfolds2, random_state=None, shuffle=False)\n",
    "    for train_index2, valid_index in skf2.split(train_images1):\n",
    "        print(j)\n",
    "        learning_rate = learning_steps[j]\n",
    "        \n",
    "        train_images = np.asarray([train_images1[i] for i in train_index2])\n",
    "        train_labels = np.asarray([train_labels1[i] for i in train_index2])\n",
    "        valid_images = np.asarray([train_images1[i] for i in valid_index])\n",
    "        valid_labels = np.asarray([train_labels1[i] for i in valid_index])\n",
    "                \n",
    "        graph = tf.Graph()\n",
    "        \n",
    "        with graph.as_default():\n",
    "\n",
    "            model_input = tf.placeholder(tf.float32, shape=(None, image_size, image_size, num_channels))\n",
    "            model_output = tf.placeholder(tf.float32, shape=(None, num_classes))\n",
    "            model_training = tf.placeholder(tf.bool)\n",
    "\n",
    "            current_epoch = tf.Variable(0, trainable=False)\n",
    "            #Define Parameters\n",
    "\n",
    "            if learn_decay:\n",
    "                #learning_rate = tf.train.exponential_decay(learning_steps[j], current_epoch, decay_steps=num_epochs, decay_rate=0.03)\n",
    "                learning_rate = tf.train.exponential_decay(\n",
    "                                  learning_steps[j],                # Base learning rate.\n",
    "                                  current_epoch,  # Current index into the dataset.\n",
    "                                  num_epochs,          # Decay step.\n",
    "                                  0.9,                # Decay rate.\n",
    "                                  staircase=True)\n",
    "            else:\n",
    "                learning_rate = learning_steps[j]\n",
    "                print(\"LEARNING RATE:\" + str(learning_rate))\n",
    "\n",
    "            if momentum_inc:\n",
    "                m_min = 0.9\n",
    "                m_max = 0.99\n",
    "                momentum_rate = m_min + (m_max - m_min) * (current_epoch / num_epochs)\n",
    "            else:\n",
    "                momentum = momentum_amount\n",
    "\n",
    "            #Model Name\n",
    "            model_name = \"ConvNet-CV\" + \"_Opt\" + str(i) + str(\"_LRN\") + str(learning_steps[j]) + \"_Epochs\" + str(num_epochs)\n",
    "            model_filename = model_name + \".ckpt\"\n",
    "            model_directory = os.getcwd() + \"/Models5/\" + model_name\n",
    "            model_path = model_directory + \"/\" + model_filename\n",
    "\n",
    "            # get model\n",
    "            if(use_ConvNet):\n",
    "                with tf.variable_scope(model_name):\n",
    "                    model_predictions = createConvNetwork(model_input, model_training)\n",
    "            else:\n",
    "                with tf.variable_scope(model_name):\n",
    "                    model_predictions = createSimpleNetwork(model_input)\n",
    "\n",
    "            model_loss = tf.reduce_mean(tf.square(model_predictions - model_output))\n",
    "\n",
    "            if momentum_steps[i] == 0:\n",
    "                model_optimizer = tf.train.MomentumOptimizer(learning_rate, momentum_rate, use_nesterov=True).minimize(model_loss, global_step = current_epoch)\n",
    "            elif momentum_steps[i] == 1:\n",
    "                model_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_loss, global_step = current_epoch)\n",
    "            else:\n",
    "                model_optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_loss, global_step = current_epoch)\n",
    "\n",
    "\n",
    "\n",
    "        trainModel(train_images, train_labels, valid_images, valid_labels, test_images, test_labels, num_epochs, batch_size, model_name, flip, noise, bright, rotate)\n",
    "\n",
    "        j = j + 1\n",
    "    i = i + 1"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
